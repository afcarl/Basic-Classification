\documentclass[a4paper,11pt]{article}
\begin{document}
\title{ML CS726 Fall'11 Project 1 Basic Classifiers}
\author{Angjoo Kanazawa, Ran Liu, Austin Myers}
\maketitle


\section{Writeup}
\subsection{Warm up}
\textbf{WU1}:\textsf{Why is this computation equivalent to computing classification
accuracy?}\\
\subsection{Decision Tree}
\textbf{WU2}:\textsf{ We should see training accuracy (roughly) going down and
test accuracy (roughly) going up.  Why does training accuracy tend to
go down?  Why is test accuracy not monotonically
increasing?}\\

Training accuracy tends to go down because we're training the
algorithm on that data with answers. The design of the tree is s.t. we
minimize the training error. However the test accuracy is not
monotonically increasing because there maybe examples that the
algoirthm is not ready for. In another words the more the accuracy
goes up on the training set the more algorithm will overfit on that
specific training set and thus not generalize well to the unseen test set.\\

\noindent
\textbf{WU3:} \textsf{You should see training accuracy monotonically increasing
and test accuracy making a (wavy) hill.  Which of these
is guaranteed to happen a which is just something we might
expect to happen?  Why?}\\

We can guarantee that the training accuracy will monotonically
increase. This is because the deeper we go, the better our tree will
fit the training data. By construction when the tree is complete as
long as the training set is consistent we will have 0 error. We should
expect the test set to make a wavy hill because we can't gurantee that
our training set is representative of the test set, or the entire
distribution datasets are taken from. So if we overfit it will go up,
but we might focus on the right feature, or hope to do so, to increase
the accuracy on the test set. But we can never guarantee that the
accuracy of the test set will monotonically increase.\\

\noindent
\textbf{WU4}: \textsf{Train a decision tree on the CG data with a maximum depth
of 3.  If you look in \texttt{datasets.CFTookCG.courseIds}
and \texttt{courseNames} you'll find the corresponding course for
each feature.  The first feature is a constant-one "bias" feature.
Draw out the decision tree for this classifier, but put in the actual
course names/ids as the features.  Interpret this tree: do these
courses seem like they are actually indicative of whether someone
might take CG?}

\subsection{KNN}
\textbf{WU5:} \textsf{For the course recommender data, generate train/test
curves for varying values of K and epsilon (you figure out what are
good ranges, this time).  Include those curves: do you see evidence of
overfitting and underfitting?  Next, using K=5, generate learning
curves for this data.}


\subsection{Perceptron}
\textbf{WU6:} \textsf{Take the best perceptron you've been able to find
  so far on the AI data.  Look at the top five positive weights (those
  with highest value) and top five negative weights (those with lowest
  value).  Which features do these correspond to?  Can you explain why
  these might get these features as the "most indicative"?  Why is it
  hard to interpret "large weight" as "most indicative"?  How do these
  large weighted features compare to the features selected by the
  decision tree?}


\end{document}
